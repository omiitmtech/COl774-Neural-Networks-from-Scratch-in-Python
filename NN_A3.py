# -*- coding: utf-8 -*-
"""A3_Neural_Network_Partd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ECwNB57mUDcLtVyzQ_L3bOPckrgMer8
"""

# from google.colab import drive
# drive.mount('/content/drive/')

# root_path = '/content/drive/My Drive/alphabet/'

import pandas as pd
import numpy as np
from sklearn.neural_network import MLPClassifier
import sklearn.metrics as metrics
import matplotlib.pyplot as plt
from tqdm import tqdm
import math
import ast
import sys
import time

# path to the data files
# training_file_path = root_path+'train.csv'
# test_file_path = root_path+'test.csv'

training_file_path = sys.argv[1]
test_file_path     = sys.argv[2]
quet_no            = sys.argv[3] 

#function to read the data and convert into numpy arrays
def prepare_data(file_path):
    train_data = np.array(pd.read_csv(file_path, header=None))
    x_train = train_data[:,0:784]
    y_train = train_data[:,784:785]
    y_train = y_train.reshape(len(y_train),)
    
    return x_train,y_train

#function to convert labels into one hot encoding form
def one_hot_encoder(v_labels):
    labels = np.zeros((len(v_labels),len(np.unique(v_labels))))
    rows = np.arange(v_labels.size)
    labels[rows,v_labels] = 1
    return labels

#read the data from files
x_train,y_train = prepare_data(training_file_path)
X = x_train
X = (X*1.0)/255
Y = one_hot_encoder(y_train)
# Y = y_train
print('Shape of X :',X.shape)
print('Shape of Y :',Y.shape)

test_features, test_y = prepare_data(test_file_path)
test_features = (test_features*1.0)/255
test_labels = one_hot_encoder(test_y)

def mean_sq_error(labels, predictions): 
    diff = np.subtract(labels, predictions)
    sq_diff = np.square(diff)
    error_vector = np.sum(sq_diff,axis=1)/2
    vector_sum = np.sum(error_vector)
    error = (vector_sum*1.0)/vector_sum.size
    return error

def activation_func(z,activation_fn,deriv=False):
    if activation_fn == 'sigmoid':
        if deriv == True:
            return np.multiply(z,1-z)
        else:
            return 1/(1 + np.exp(-z))
    if activation_fn == 'relu':
        if deriv == True:
            return (z>0)*1
        else:
            input_rows = z
            for r in input_rows:
                r[r < 0] = 0
            return input_rows

def initialize_layers(nodes_hidden_layer,no_of_features,no_of_classes,batch_size,initial_weight):
    
    layers = []
    no_layers = len(nodes_hidden_layer)
    first_layer = 0
    
    #first hidden layer as number of neurons = no_of_features + 1
    for i in range(no_layers+1):
        layer = {}
        if first_layer == 0:
            no_weights = no_of_features+1
            first_layer +=1
        else :
            no_weights = nodes_hidden_layer[i-1]+1
        
        if i == no_layers :
            no_neurons = no_of_classes
        else :
            no_neurons = nodes_hidden_layer[i]
        
        layer["node_count"] = no_neurons
        layer["weights"] = np.random.uniform(-initial_weight, initial_weight, size=(no_neurons, no_weights))
        layer["net_input"] = np.zeros((batch_size,no_weights))
        print('layer no :',i,'No of neurons :',no_neurons,'No of weights :',no_weights,layer["weights"].shape)
        layers.append(layer)
    
    return layers

def forward_pass(input_data, activation_fn, layers, batch_size):
    
    total_layers = len(layers) #no. of layers in the network
    
    #set x0 =1 for bias, concate 1's to input data 
    bias_ones = np.ones(input_data.shape[0])
    input_data = np.c_[bias_ones,input_data]
    layers[0]["net_input"] = input_data #whole batch is copied to net_input
    tot_lay = total_layers-1
    
    for i in range(tot_lay): #iterate through all the layers 
        ith_layer_weight = layers[i]["weights"]
        ith_layer_net_input = layers[i]["net_input"].T
        
        next_net_input = np.dot(ith_layer_weight,ith_layer_net_input)
        #it calculates W*X.T + b
        
        next_net_input = activation_func(next_net_input,activation_fn)
        
        #set x0=1 for next_net_input
        layers[i+1]["net_input"] = next_net_input.T #it will be input for next layer
        #1's added to the next layer input
        ones_shape = layers[i+1]["net_input"]
        i_plus_layer_ones = np.ones(ones_shape.shape[0])
        
        layers[i+1]["net_input"] = np.c_[i_plus_layer_ones,ones_shape]

def back_propagation(labels, predictions, last_layer, layers, learning_rate, activation_fn, net):
    
    batch_size = predictions.shape[0]
    
    derv = predictions*(1-predictions) #multiply element by element
    difference = labels - predictions
    delta_L = np.multiply(difference, derv)
    delta_trans = delta_L.T
    last_lay_inp = layers[last_layer]["net_input"]
    delta_w = np.dot(delta_trans,last_lay_inp )
    #net_input is the output of the previous layer
    last_lay_weight = layers[last_layer]["weights"]
    
    learn_rate_delta_w = learning_rate * delta_w
    divr = learn_rate_delta_w/batch_size
    
    
    
    #back propagation in hidden layers
    n = last_layer-1
    
    updated_weights = last_lay_weight + divr
    
    for i in range(n , -1, -1):
        
        y = layers[i+1]["net_input"]
        
        derv = activation_func(y,activation_fn,deriv=True)
        
        layers[i+1]["weights"] = updated_weights
        
        i_plus_lay_weights = layers[i+1]["weights"][:,1:]
        
        temp = np.dot(delta_L , i_plus_lay_weights)
        
        
        delta_L = np.multiply(temp, derv[:,1:])
        i_lay_weights = layers[i]["net_input"]
        delta_w = np.dot(delta_L.T , i_lay_weights)
        
        i_lay_weight = layers[i]["weights"]
        learn_rate_delta_w = learning_rate * delta_w
            
        updated_weights = i_lay_weight + (learn_rate_delta_w/batch_size)
    layers[0]["weights"] = updated_weights

def train_per_batch(layers, features, labels, activation_fn, learning_rate, nt):
    #features is a subset of x, for example 100x784 matrix
    #labels is subset of y, for example 100x1 matrix of hard encoded 
    batch_size = 100
    forward_pass(features, activation_fn, layers, batch_size) #forward propagation
    
    #get the prediction of output layer
    last_layer = len(layers) - 1
    last_layer_input = layers[last_layer]["net_input"]
    last_layer_weight = layers[last_layer]["weights"].T
    z = np.dot(last_layer_input,last_layer_weight)
#     predictions = sigmoid(z)
    predictions = activation_func(z,'sigmoid')
    #output of lastlayer
    #compute mean squared error
    error = mean_sq_error(labels, predictions)
    #back propagate the error to update weights
    back_propagation(labels, predictions, last_layer, layers, learning_rate, activation_fn,batch_size )
    
    return error

def get_accuracy(layers, features, labels, activation_fn):
    
    batch_size = 100
    forward_pass(features, activation_fn, layers,batch_size)
    
    #get the prediction
    last_layer = len(layers) - 1
    
    last_layer_inp = layers[last_layer]["net_input"]
    last_layer_weight = layers[last_layer]["weights"].T
    mult = np.dot(last_layer_inp, last_layer_weight)
    predictions = activation_func(mult,'sigmoid')
    
    total_count = len(predictions)
    
    for i in range(0,total_count):  
        
        maximum_val = np.max(predictions[i])
        true_val = predictions[i] >= maximum_val
        predictions[i] = true_val.astype(int)

    
    print('No of total values Match :',total_count)
    crct_result = np.all(labels == predictions, axis=1)
    print('number of correct match :',np.sum(crct_result))
    correct_count = np.sum(crct_result)
    
    accuracy = (correct_count*100.0)/total_count
    print("Accuracy: ", accuracy)
    return round(accuracy,2)

def train(nodes_each_layer, batch_size, train_features, train_labels, num_epochs, activation_fn, learning_rate, learning_type='fixed',initial_weight=0.1):
    no_of_features = train_features.shape[1] # no. of attributes
    no_of_classes = train_labels.shape[1] # no. of classes
    print('classes :',no_of_classes)
    layers = initialize_layers(nodes_each_layer,no_of_features,no_of_classes, batch_size,initial_weight)
    dataset_size = len(train_features)
    n0 = 0.5
    for j in tqdm(range(1,num_epochs+1)):
        if learning_type =='adaptive':
          learning_rate = n0/np.sqrt(j)
        i = 0
        while i+batch_size <= dataset_size :
            train_features_slice = train_features[i:i+batch_size]
            train_labels_slice = train_labels[i:i+batch_size]
            error = train_per_batch(layers, train_features_slice,train_labels_slice , activation_fn, learning_rate, 2.0)
            i +=batch_size
    print('Error :',error) 
    return layers

def plot_test_accuracy(title_text,X_label,Y_label,X_Axis,test_accuracies,training_accuracies):
  if len(test_accuracies) != 1:
    plt.plot(X_Axis, test_accuracies, color='g',label="Test Accuracy")
    plt.scatter(X_Axis,test_accuracies)
  plt.plot(X_Axis, training_accuracies, color='orange',label="Training Accuracy")
  plt.scatter(X_Axis,training_accuracies)
  plt.xlabel(X_label)
  plt.ylabel(Y_label)
  plt.title(title_text)
  # plt.ylim(0.78,0.82)
  plt.xticks(X_Axis)
  plt.legend(loc='lower right')
  plt.show()

#----Que_2(B)
def que_b_c(learning_type='fixed'):

  hidden_layers = [1,5,10,50,100]
  training_accuracies = []
  test_accuracies = []
  time_taken = []
  for neurons in hidden_layers:
      t0 = time.time()
      layers = train([neurons], 100, X, Y, 1500, 'sigmoid',0.1,learning_type)
      time_taken.append(round(time.time()-t0))
      training_accuracies.append(get_accuracy(layers, X, Y, 'sigmoid'))
      test_accuracies.append(get_accuracy(layers, test_features, test_labels, 'sigmoid'))
  
  print('Training Accuracies :', training_accuracies)
  print('Testing Accuracies :', test_accuracies)
  print('Time Taken :', time_taken)
  #plot accuracies
  plot_test_accuracy('No of Neurons in Hidden Layer V/S Accuracies','No. of Neurons in Hidden layer -->','Accuracies',hidden_layers,test_accuracies,training_accuracies)
  #plot time taken to train the model
  plot_test_accuracy('No of Neurons in Hidden Layer V/S Time Taken by Model','No. of Neurons in Hidden layer -->','Time in Seconds -->',hidden_layers,[0],time_taken)

def mlp_classifier(features,labels,epochs,learning_rate,activation_fn,momentum_fn=0.9):
  mlp_clf = MLPClassifier((100,100),activation=activation_fn,solver='sgd',batch_size=100,learning_rate='invscaling',learning_rate_init=learning_rate,shuffle=True,random_state=1, max_iter = epochs,momentum=momentum_fn)
  mlp_clf.fit(features,labels)
  return mlp_clf

def mlp_cal_accuracy(classifier,features, labels):
  prob_matrix = classifier.predict_proba(features)
  prob_array =  np.argmax(prob_matrix,axis=1)
  val_match = (prob_array == labels)
  print('Matched :',np.count_nonzero(val_match),'Total examples :',len(val_match))
  print(np.count_nonzero(val_match)/len(val_match))

#-----------Question_(a)--------------------
# python3 NN_A3.py 'Part2_data/train.csv' 'Part2_data/test.csv' 'd' [10] 100 100 'sigmoid' 0.5 fixed 0.1
if quet_no == 'a':

    print('--neural network building starts --')
    print ("provides the parameter :train(nodes_each_layer, batch_size, train_features, train_labels, num_epochs, activation_fn, learning_rate, learning_type='fixed',initial_weight=0.1)")
    v_hidden_layers = sys.argv[4]
    v_nodes_each_layer = ast.literal_eval(v_hidden_layers)
    print('v_nodes_each_layer :',v_nodes_each_layer)
    v_batch_size = int(sys.argv[5])
    v_train_features = X
    v_train_labels = Y
    v_num_epochs = int(sys.argv[6])
    v_activation_fn = sys.argv[7]
    v_learning_rate = float(sys.argv[8])
    v_learning_type = sys.argv[9]
    v_initial_weight = float(sys.argv[10])
    NN = train([10], v_batch_size, X, Y, v_num_epochs,v_activation_fn, v_learning_rate, v_learning_type,v_initial_weight)
    get_accuracy(NN, X, Y, v_activation_fn)
    get_accuracy(NN, test_features, test_labels, v_activation_fn)
#-----------Question_(b)--------------------
if quet_no == 'b':
	que_b_c('fixed')

#-----------Question_(c)-------------------- 
if quet_no == 'c':
	que_b_c('adaptive')


#Que2(d)------- 
if quet_no == 'd':

	print('Activation function : sigmoid')
	t0 = time.time()
	layers = train([100,100], 100, X, Y, 1500, 'sigmoid',0.5,'adaptive')
	print('time to train the model with sigmoid activation function',(time.time()-t0)/60)
	get_accuracy(layers, X, Y, 'sigmoid')
	get_accuracy(layers, test_features, test_labels, 'sigmoid')
	print('Activation function : Relu')
	t0 = time.time()
	layers = train([100,100], 100, X, Y, 1000, 'relu',0.5,'adaptive',0.01)
	print('time to train the model with relu activation function',(time.time()-t0)/60)
	get_accuracy(layers, X, Y, 'relu')
	get_accuracy(layers, test_features, test_labels, 'relu')

#--------Que2(E)
if quet_no == 'e':
	print('--part_e, sigmoid--')
	t0 = time.time()
	mlp_clf = mlp_classifier(X,Y,1500,0.5,'logistic')
	print('time to train the model :',time.time()-t0)
	mlp_cal_accuracy(mlp_clf,X, y_train)
	mlp_cal_accuracy(mlp_clf,test_features, test_y)

	print('--part_e, relu--')
	t0 = time.time()
	mlp_clf = mlp_classifier(X,Y,1500,0.5,'relu',0)
	print('time to train the model :',time.time()-t0)
	mlp_cal_accuracy(mlp_clf,X, y_train)
	mlp_cal_accuracy(mlp_clf,test_features, test_y)

